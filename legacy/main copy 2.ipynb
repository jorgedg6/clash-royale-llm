{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "505b6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import re\n",
    "from random import shuffle\n",
    "import random\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8d921ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from card_utils import read_all_game_data, Card, PlayerCards, PlayerStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f482727",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "304bc2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deck_rate_fetcher import fetch_deck_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75129564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ae66436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from openai import AsyncAzureOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1813383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_cards_list, player_cards_list, player_stats, player_tags_list = read_all_game_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e54480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_decks_per_card = pd.read_json(\"data/top_decks_per_card.json\").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d2a5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for card_name in top_decks_per_card:\n",
    "    for key in top_decks_per_card[card_name].keys():\n",
    "        deck_list = top_decks_per_card[card_name][key].split(',')\n",
    "        shuffle(deck_list)\n",
    "        top_decks_per_card[card_name][key] = deck_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "76e33d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LLMModel:\n",
    "    name: str\n",
    "    instance: AzureChatOpenAI | AsyncAzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "54de7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_gpt3 = LLMModel(\n",
    "    name=\"gpt-35-turbo\",\n",
    "    instance=AzureChatOpenAI(\n",
    "        model_name=\"gpt-35-turbo\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "llm_gpt4o = LLMModel(\n",
    "    name=\"gpt-4o\",\n",
    "    instance=AzureChatOpenAI(\n",
    "        model_name=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "llm_gpt5 = LLMModel(\n",
    "    name=\"gpt-5-chat\",\n",
    "    instance=AzureChatOpenAI(\n",
    "        model_name=\"gpt-5-chat\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "65334696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAIMessage:\n",
    "    \"\"\"Clase simple para empaquetar la respuesta con un atributo .content\"\"\"\n",
    "    def __init__(self, content, finish_reason=None):\n",
    "        self.content = content\n",
    "        self.finish_reason = finish_reason\n",
    "\n",
    "class OpenAIClientAdapter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: AsyncAzureOpenAI,\n",
    "        model_name: str,\n",
    "        temperature: float = 0,\n",
    "        max_tokens: int = 2048,\n",
    "        request_timeout: int = 300\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.request_timeout = request_timeout\n",
    "        print(f\"Adaptador creado para el modelo: {self.model_name}\")\n",
    "\n",
    "    def _convert_lc_messages_to_dict(self, messages):\n",
    "        output = []\n",
    "        for msg in messages:\n",
    "            role = msg.type\n",
    "            if role == \"human\":\n",
    "                role = \"user\"\n",
    "            elif role == \"ai\":\n",
    "                role = \"assistant\"\n",
    "            output.append({\"role\": role, \"content\": msg.content})\n",
    "        return output\n",
    "\n",
    "    async def ainvoke(self, messages):\n",
    "        try:\n",
    "            dict_messages = self._convert_lc_messages_to_dict(messages)\n",
    "            \n",
    "            resp = await self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=dict_messages,\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens,\n",
    "                extra_body={\"max_output_tokens\": self.max_tokens},\n",
    "                stop=None,\n",
    "                timeout=self.request_timeout,\n",
    "            )\n",
    "\n",
    "            choice = resp.choices[0]\n",
    "            content = choice.message.content\n",
    "            finish_reason = getattr(choice, \"finish_reason\", None)\n",
    "\n",
    "            if finish_reason and finish_reason != \"stop\":\n",
    "                print(f\"[{self.model_name}] finish_reason={finish_reason}\")\n",
    "\n",
    "            return SimpleAIMessage(content=content, finish_reason=finish_reason)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en el adaptador de OpenAI ({self.model_name}): {e}\")\n",
    "            return SimpleAIMessage(content=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95a8819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando clientes y adaptadores...\n",
      "Adaptador creado para el modelo: grok-4-fast-non-reasoning\n",
      "Adaptador creado para el modelo: DeepSeek-V3.1\n",
      "Adaptador creado para el modelo: Llama-3.3-70B-Instruct\n"
     ]
    }
   ],
   "source": [
    "print(\"Creando clientes y adaptadores...\")\n",
    "\n",
    "async_openai_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=dotenv.get_key(dotenv.find_dotenv(), \"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=dotenv.get_key(dotenv.find_dotenv(), \"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")\n",
    "\n",
    "llm_grok_adapter = LLMModel(\n",
    "    name=\"grok-4-fast-non-reasoning\",\n",
    "    instance=OpenAIClientAdapter(\n",
    "        client=async_openai_client,\n",
    "        model_name=\"grok-4-fast-non-reasoning\",\n",
    "        temperature=0\n",
    "    )\n",
    ")\n",
    "\n",
    "llm_deepseek_adapter = LLMModel(\n",
    "    name=\"DeepSeek-V3.1\",\n",
    "    instance=OpenAIClientAdapter(\n",
    "        client=async_openai_client,\n",
    "        model_name=\"DeepSeek-V3.1\",\n",
    "        temperature=0\n",
    "    )\n",
    ")\n",
    "\n",
    "llm_llama_adapter = LLMModel(\n",
    "    name=\"Llama-3.3-70B-Instruct\",\n",
    "    instance=OpenAIClientAdapter(\n",
    "        client=async_openai_client,\n",
    "        model_name=\"Llama-3.3-70B-Instruct\",\n",
    "        temperature=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b44cf5f",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b053b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-28 01:33:24,140 - root - INFO - Logger configurado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import sys\n",
    "\n",
    "LOG_LEVEL = logging.INFO \n",
    "\n",
    "logging.basicConfig(\n",
    "    level=LOG_LEVEL,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"logs/app_run{}.log\".format(time.strftime('%Y%m%d_%H%M%S'))), \n",
    "        logging.StreamHandler(sys.stdout) \n",
    "    ],\n",
    "    force=True \n",
    ")\n",
    "\n",
    "logging.info(\"Logger configurado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eae99076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando prompts desde 'prompts'...\n",
      " - Cargado: human_prompt_context_multistep\n",
      " - Cargado: system_prompt\n",
      " - Cargado: human_prompt_no_context_english\n",
      " - Cargado: human_prompt_context_semi_lite\n",
      " - Cargado: human_prompt_context_lite_english\n",
      " - Cargado: human_prompt_no_context\n",
      " - Cargado: human_prompt_no_context_lite_english\n",
      " - Cargado: human_prompt_no_context_lite\n",
      " - Cargado: human_prompt_context_multistep_english\n",
      " - Cargado: human_prompt_context_english\n",
      " - Cargado: human_prompt_context_lite\n",
      " - Cargado: system_prompt_english\n",
      " - Cargado: human_prompt_no_context_semi_lite\n",
      " - Cargado: human_prompt_no_context_semi_lite_english\n",
      " - Cargado: human_prompt_context\n",
      " - Cargado: human_prompt_context_semi_lite_english\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_template_text(text):\n",
    "    \"\"\"\n",
    "    Aplica la lógica de formateo específica de tu notebook:\n",
    "    1. Escapa las llaves originales para que no rompan el .format().\n",
    "    2. Aplana los saltos de línea y escapa comillas.\n",
    "    3. Reemplaza los marcadores especiales $ y % por llaves de formato reales.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    text = text.replace(\"\\n\", \" \").replace('\"', '\\\\\"')\n",
    "    text = text.replace(\"$\", \"{\").replace(\"%\", \"}\")\n",
    "    return text\n",
    "\n",
    "def load_prompts_from_dir(directory=\"prompts\"):\n",
    "    prompts_dict = {}\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Advertencia: El directorio '{directory}' no existe.\")\n",
    "        return prompts_dict\n",
    "\n",
    "    print(f\"Cargando prompts desde '{directory}'...\")\n",
    "    \n",
    "    files = [f for f in os.listdir(directory) if f.endswith(\".txt\")]\n",
    "    \n",
    "    for filename in files:\n",
    "        # El nombre de la clave será el nombre del archivo sin extensión\n",
    "        # Ej: \"human_prompt_context_lite.txt\" -> \"human_prompt_context_lite\"\n",
    "        key_name = os.path.splitext(filename)[0]\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # Mantenemos la lógica original:\n",
    "        # Si el archivo tiene \"system\" en el nombre, se carga crudo.\n",
    "        # Si no (es un human/template), se procesa para .format().\n",
    "        if \"system\" in key_name:\n",
    "            prompts_dict[key_name] = content\n",
    "        else:\n",
    "            prompts_dict[key_name] = process_template_text(content)\n",
    "            \n",
    "        print(f\" - Cargado: {key_name}\")\n",
    "\n",
    "    return prompts_dict\n",
    "\n",
    "# Ejecución\n",
    "prompts_data = load_prompts_from_dir(\"prompts\")\n",
    "\n",
    "# Extracción de variables individuales (Opcional, por compatibilidad hacia atrás)\n",
    "# Si tu código más abajo usa explícitamente estas variables, puedes asignarlas así:\n",
    "system_prompt = prompts_data.get(\"system_prompt_english\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "72b6d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# --- Estructuras de Datos ---\n",
    "\n",
    "@dataclass\n",
    "class PromptConfig:\n",
    "    name: str  # ej: \"no_context_lite\"\n",
    "    template: str\n",
    "    requires_context: bool\n",
    "\n",
    "@dataclass\n",
    "class TaskPayload:\n",
    "    execution_id: str\n",
    "    user_id: str\n",
    "    llm_name: str\n",
    "    prompt_name: str\n",
    "    prompt_text: str\n",
    "    deck_original: List[str]      # Nombres de cartas originales (las 4 fijas)\n",
    "    cards_available: List[str]    # Pool disponible\n",
    "    cards_deleted: List[str]      # Las 4 que borramos (para validar si el LLM las adivina)\n",
    "\n",
    "# --- Utilidad de Parseo de JSON (Solo extracción) ---\n",
    "\n",
    "def extract_json_payload(text: str) -> Optional[Dict]:\n",
    "    \"\"\"Intenta extraer y parsear un bloque JSON de un string sucio.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        # Intento 1: Buscar bloques de código markdown ```json ... ```\n",
    "        m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", text, flags=re.IGNORECASE)\n",
    "        candidate = m.group(1).strip() if m else text.strip()\n",
    "\n",
    "        # Intento 2: Buscar los límites de llaves {} o corchetes []\n",
    "        first_brace = candidate.find(\"{\")\n",
    "        first_bracket = candidate.find(\"[\")\n",
    "        starts = [i for i in (first_brace, first_bracket) if i != -1]\n",
    "        \n",
    "        if not starts:\n",
    "            return None\n",
    "\n",
    "        start = min(starts)\n",
    "        end = max(candidate.rfind(\"}\"), candidate.rfind(\"]\"))\n",
    "        \n",
    "        if end == -1 or end < start:\n",
    "            return None\n",
    "            \n",
    "        candidate = candidate[start:end+1]\n",
    "        return json.loads(candidate)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cdd813e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_prompt_file(string, user_id):\n",
    "    with open(f\"final_prompts_lite/user_{user_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8956854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_human_prompt(human_prompt_template, top_decks, available_cards, selected_cards):\n",
    "    rendered_human_prompt = human_prompt_template.format(\n",
    "        TOP_DECKS=top_decks,\n",
    "        CARTAS_DISPONIBLES=available_cards,\n",
    "        CARTAS_SELECCIONADAS=selected_cards\n",
    "    )\n",
    "    return rendered_human_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cd1649cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Función Auxiliar: Barajar y Borrar Cartas ---\n",
    "def shuffle_and_remove_cards(player_cards_list, num_to_remove=4):\n",
    "    updated_player_cards_list = []\n",
    "    for player_cards in player_cards_list:\n",
    "        available_copy = list(player_cards.available_cards)\n",
    "        selected_copy = list(player_cards.deck_cards)\n",
    "\n",
    "        shuffle(available_copy)\n",
    "        shuffle(selected_copy)\n",
    "\n",
    "        to_delete = selected_copy[-num_to_remove:]\n",
    "        selected_for_prompt = selected_copy[:-num_to_remove]\n",
    "\n",
    "        updated_player_cards = PlayerCards(\n",
    "            tag=player_cards.tag,\n",
    "            available_cards=available_copy,\n",
    "            deck_cards=selected_for_prompt,\n",
    "            deleted_cards=to_delete\n",
    "        )\n",
    "        updated_player_cards_list.append(updated_player_cards)\n",
    "    return updated_player_cards_list\n",
    "\n",
    "# --- Lógica 1: Construcción del Prompt ---\n",
    "def build_prompt_text(config: PromptConfig, player_cards: Any, top_decks_map: Dict) -> str:\n",
    "    available_json = json.dumps([c.name for c in player_cards.available_cards])\n",
    "    deck_json = json.dumps([c.name for c in player_cards.deck_cards])\n",
    "    \n",
    "    if config.requires_context:\n",
    "        # Construcción del contexto XML\n",
    "        string_top_decks = \"<TopDecks>\\n\"\n",
    "        for card in player_cards.deck_cards:\n",
    "            card_name = card.name\n",
    "            string_top_decks += f'  <CardGroup name=\"{card_name}\">\\n'\n",
    "            decks = top_decks_map.get(card_name, {})\n",
    "            for d_key, d_list in decks.items():\n",
    "                string_top_decks += \"    <Deck>\\n\"\n",
    "                for c_name in d_list:\n",
    "                    string_top_decks += f'      <Card>{c_name.strip()}</Card>\\n'\n",
    "                string_top_decks += \"    </Deck>\\n\"\n",
    "            string_top_decks += \"  </CardGroup>\\n\"\n",
    "        string_top_decks += \"</TopDecks>\"\n",
    "        \n",
    "        return config.template.format(\n",
    "            TOP_DECKS=string_top_decks,\n",
    "            CARTAS_DISPONIBLES=available_json,\n",
    "            CARTAS_SELECCIONADAS=deck_json\n",
    "        )\n",
    "    else:\n",
    "        return config.template.format(\n",
    "            CARTAS_DISPONIBLES=available_json,\n",
    "            CARTAS_SELECCIONADAS=deck_json\n",
    "        )\n",
    "\n",
    "# --- Lógica 2: Procesamiento de una Tarea (LLM Only) ---\n",
    "async def process_single_task(llm_instance: Any, task: TaskPayload, file_lock: asyncio.Lock, output_file: str):\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt), \n",
    "        HumanMessage(content=task.prompt_text)\n",
    "    ]\n",
    "    \n",
    "    response_content = None\n",
    "    parsed_json = None\n",
    "    error_msg = None\n",
    "    \n",
    "    try:\n",
    "        ai_msg = await llm_instance.instance.ainvoke(messages)\n",
    "        response_content = ai_msg.content\n",
    "        \n",
    "        parsed_data = extract_json_payload(response_content)\n",
    "        \n",
    "        # Normalizar respuesta a {\"seleccion\": [...]}\n",
    "        if parsed_data:\n",
    "            if isinstance(parsed_data, list):\n",
    "                 parsed_json = {\"seleccion\": parsed_data}\n",
    "            elif isinstance(parsed_data, dict):\n",
    "                 # Si viene {\"data\": ...} o {\"seleccion\": ...}\n",
    "                 if \"seleccion\" in parsed_data:\n",
    "                     parsed_json = parsed_data\n",
    "                 elif \"data\" in parsed_data:\n",
    "                     parsed_json = {\"seleccion\": parsed_data[\"data\"]}\n",
    "                 else:\n",
    "                     parsed_json = {\"seleccion\": parsed_data} # Fallback\n",
    "            else:\n",
    "                 parsed_json = {\"seleccion\": []} # Formato irreconocible\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        logging.error(f\"Error en tarea {task.execution_id}: {e}\")\n",
    "\n",
    "    # Registro final\n",
    "    result_record = {\n",
    "        \"execution_id\": task.execution_id,\n",
    "        \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "        \"user_id\": task.user_id,\n",
    "        \"llm\": task.llm_name,\n",
    "        \"prompt_type\": task.prompt_name,\n",
    "        \"original_deck\": task.deck_original,\n",
    "        \"deleted_cards\": task.cards_deleted,\n",
    "        \"raw_response\": response_content,\n",
    "        \"parsed_selection\": parsed_json.get(\"seleccion\") if parsed_json else None,\n",
    "        \"is_parsed\": parsed_json is not None,\n",
    "        \"error\": error_msg\n",
    "    }\n",
    "\n",
    "    async with file_lock:\n",
    "        with open(output_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(result_record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# --- Lógica 3: Pipeline Principal de Generación ---\n",
    "async def run_generation_pipeline(\n",
    "    player_cards_list: List[Any],\n",
    "    models: List[Any],\n",
    "    prompts_config: List[PromptConfig],\n",
    "    user_range: tuple = (0, None), \n",
    "    batch_size: int = 10,\n",
    "    output_filename: str = \"generation_results.jsonl\"\n",
    "):\n",
    "    # 1. Preparar Usuarios (Shuffle + Slice Customizado)\n",
    "    start, end = user_range\n",
    "    active_players = shuffle_and_remove_cards(player_cards_list, num_to_remove=4)\n",
    "    \n",
    "    if end is None:\n",
    "        target_players = active_players[start:]\n",
    "    else:\n",
    "        target_players = active_players[start:end]\n",
    "        \n",
    "    logging.info(f\"Procesando {len(target_players)} usuarios (Rango: {start}-{end if end else 'End'}).\")\n",
    "\n",
    "    # 2. Generar Tareas (Usuario x Modelo x Prompt)\n",
    "    all_tasks = []\n",
    "    \n",
    "    for player in target_players:\n",
    "        player_tag = player.tag[1:]\n",
    "        # Pre-computar prompt text para guardarlo si fuera necesario debugging\n",
    "        current_deck_names = [c.name for c in player.deck_cards]\n",
    "        deleted_names = [c.name for c in player.deleted_cards]\n",
    "        available_names = [c.name for c in player.available_cards]\n",
    "        \n",
    "        for llm in models:\n",
    "            for prompt_cfg in prompts_config:\n",
    "                \n",
    "                try:\n",
    "                    p_text = build_prompt_text(prompt_cfg, player, top_decks_per_card)\n",
    "                    \n",
    "                    task = TaskPayload(\n",
    "                        execution_id=uuid.uuid4().hex,\n",
    "                        user_id=player_tag,\n",
    "                        llm_name=llm.name,\n",
    "                        prompt_name=prompt_cfg.name,\n",
    "                        prompt_text=p_text,\n",
    "                        deck_original=current_deck_names,\n",
    "                        cards_available=available_names,\n",
    "                        cards_deleted=deleted_names\n",
    "                    )\n",
    "                    all_tasks.append((llm, task))\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error construyendo prompt para {player_tag}: {e}\")\n",
    "\n",
    "    # 3. Barajar para alternar prompts/modelos\n",
    "    random.shuffle(all_tasks)\n",
    "    logging.info(f\"Total de tareas generadas: {len(all_tasks)}\")\n",
    "    \n",
    "    # 4. Ejecutar\n",
    "    file_lock = asyncio.Lock()\n",
    "    total_batches = (len(all_tasks) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(all_tasks), batch_size):\n",
    "        batch = all_tasks[i:i + batch_size]\n",
    "        logging.info(f\"--- Batch Generación {i//batch_size + 1}/{total_batches} (Size: {len(batch)}) ---\")\n",
    "        \n",
    "        coroutines = [\n",
    "            process_single_task(llm, task, file_lock, output_filename) \n",
    "            for (llm, task) in batch\n",
    "        ]\n",
    "        \n",
    "        await asyncio.gather(*coroutines, return_exceptions=True)\n",
    "        # Breve pausa para no saturar APIs\n",
    "        await asyncio.sleep(0.5)\n",
    "\n",
    "    logging.info(f\"Pipeline de generación finalizado. Resultados en: {output_filename}\")\n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ff9ef0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utilidades de Rating (Re-declarar si se borraron arriba) ---\n",
    "score_mapping = {\n",
    "    \"RIP\": 0, \"Bad\": 1, \"Mediocre\": 2, \"Good\": 3, \"Great!\": 4, \"Godly!\": 5,\n",
    "}\n",
    "\n",
    "def process_deck_rating(rating: str) -> dict:\n",
    "    if not rating: return {}\n",
    "    parts = rating.strip().split(\" \")\n",
    "    # Manejo de error simple por si el string no tiene el formato esperado\n",
    "    try:\n",
    "        return {\n",
    "            \"Attack\": score_mapping.get(parts[1], 0),\n",
    "            \"Defense\": score_mapping.get(parts[3], 0),\n",
    "            \"Synergy\": score_mapping.get(parts[5], 0),\n",
    "            \"Versatility\": score_mapping.get(parts[7], 0),\n",
    "            \"F2P score\": score_mapping.get(parts[10], 0),\n",
    "        }\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "# --- Pipeline de Evaluación ---\n",
    "async def run_rating_pipeline(input_filename: str, output_filename: str = \"rating_results.jsonl\"):\n",
    "    logging.info(f\"Iniciando evaluación de decks desde {input_filename}...\")\n",
    "    \n",
    "    # 1. Cargar registros previos\n",
    "    records = []\n",
    "    try:\n",
    "        with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                records.append(json.loads(line))\n",
    "    except FileNotFoundError:\n",
    "        logging.error(\"No existe el archivo de generación.\")\n",
    "        return\n",
    "\n",
    "    # 2. Verificar procesados para no repetir si se corre de nuevo\n",
    "    processed_ids = set()\n",
    "    try:\n",
    "        with open(output_filename, 'r', encoding='utf-8') as f:\n",
    "             for line in f:\n",
    "                processed_ids.add(json.loads(line).get(\"execution_id\"))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    # 3. Función de evaluación individual\n",
    "    sem = asyncio.Semaphore(5) # Concurrencia limitada para DeckShop\n",
    "    \n",
    "    async def rate_entry(record):\n",
    "        eid = record[\"execution_id\"]\n",
    "        if eid in processed_ids: return\n",
    "\n",
    "        async with sem:\n",
    "            result = {\"execution_id\": eid, \"was_improved\": False}\n",
    "            \n",
    "            # Solo evaluamos si hubo parseo exitoso\n",
    "            if record.get(\"is_parsed\") and record.get(\"parsed_selection\"):\n",
    "                try:\n",
    "                    # Reconstruir mazos completos\n",
    "                    original_full = record[\"original_deck\"] + record[\"deleted_cards\"]\n",
    "                    \n",
    "                    selection = record[\"parsed_selection\"]\n",
    "                    # Asumiendo que selección son las cartas faltantes. Si el LLM devuelve todo, ajustar aquí.\n",
    "                    new_full = record[\"original_deck\"] + (selection if isinstance(selection, list) else [])\n",
    "\n",
    "                    if len(new_full) == 8:\n",
    "                        # Llamadas a DeckShop (bloqueantes -> thread pool)\n",
    "                        loop = asyncio.get_running_loop()\n",
    "                        \n",
    "                        r_orig = await loop.run_in_executor(None, fetch_deck_rating, original_full)\n",
    "                        r_new = await loop.run_in_executor(None, fetch_deck_rating, new_full)\n",
    "                        \n",
    "                        scores_orig = process_deck_rating(r_orig)\n",
    "                        scores_new = process_deck_rating(r_new)\n",
    "                        \n",
    "                        total_orig = sum(scores_orig.values())\n",
    "                        total_new = sum(scores_new.values())\n",
    "                        \n",
    "                        result.update({\n",
    "                            \"rating_status\": \"success\",\n",
    "                            \"scores_original\": scores_orig,\n",
    "                            \"scores_new\": scores_new,\n",
    "                            \"total_original\": total_orig,\n",
    "                            \"total_new\": total_new,\n",
    "                            \"was_improved\": total_new >= total_orig,\n",
    "                            \"correct_selection_count\": sum(1 for c in record[\"deleted_cards\"] if c in selection)\n",
    "                        })\n",
    "                    else:\n",
    "                        result[\"rating_status\"] = \"invalid_deck_len\"\n",
    "                except Exception as e:\n",
    "                    result[\"rating_status\"] = f\"error: {str(e)}\"\n",
    "            else:\n",
    "                result[\"rating_status\"] = \"skipped_no_parse\"\n",
    "\n",
    "            # Escribir append\n",
    "            with open(output_filename, 'a', encoding='utf-8') as f_out:\n",
    "                f_out.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    # 4. Ejecutar por lotes\n",
    "    tasks = [rate_entry(rec) for rec in records]\n",
    "    chunk_size = 10\n",
    "    total = len(tasks)\n",
    "    \n",
    "    for i in range(0, total, chunk_size):\n",
    "        await asyncio.gather(*tasks[i:i+chunk_size])\n",
    "        logging.info(f\"Evaluados {min(i+chunk_size, total)}/{total}\")\n",
    "        await asyncio.sleep(1) # Pausa amigable\n",
    "\n",
    "    logging.info(\"Evaluación finalizada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9a95912e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-28 01:33:24,196 - root - INFO - >>> INICIANDO FASE 1: GENERACIÓN LLM\n",
      "2025-11-28 01:33:25,200 - root - INFO - Procesando 2 usuarios (Rango: 0-2).\n",
      "2025-11-28 01:33:25,201 - root - INFO - Total de tareas generadas: 10\n",
      "2025-11-28 01:33:25,201 - root - INFO - --- Batch Generación 1/1 (Size: 10) ---\n",
      "2025-11-28 01:33:32,130 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-chat/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-28 01:33:32,134 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-chat/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-28 01:33:32,866 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-28 01:33:33,668 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-28 01:33:36,619 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-28 01:33:36,669 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-28 01:33:38,319 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/DeepSeek-V3.1/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-28 01:33:39,226 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/DeepSeek-V3.1/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-28 01:33:42,678 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/Llama-3.3-70B-Instruct/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-28 01:33:42,683 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/Llama-3.3-70B-Instruct/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-28 01:33:43,187 - root - INFO - Pipeline de generación finalizado. Resultados en: results/generations_20251128_013324.jsonl\n",
      "2025-11-28 01:33:43,217 - root - INFO - >>> INICIANDO FASE 2: EVALUACIÓN DECKSHOP\n",
      "2025-11-28 01:33:43,218 - root - INFO - Iniciando evaluación de decks desde results/generations_20251128_013324.jsonl...\n",
      "2025-11-28 01:33:43,221 - root - INFO - Evaluados 10/10\n",
      "2025-11-28 01:33:44,223 - root - INFO - Evaluación finalizada.\n",
      "2025-11-28 01:33:44,225 - root - INFO - Proceso completo. \n",
      "Generación: results/generations_20251128_013324.jsonl \n",
      "Ratings: results/ratings_20251128_013324.jsonl\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# --- A. Configuración de Ejecución ---\n",
    "\n",
    "# 1. Define qué prompts quieres evaluar\n",
    "prompts_to_run = [\n",
    "    PromptConfig(name=\"context_multistep_english\", template=prompts_data[\"human_prompt_context_multistep_english\"], requires_context=True),\n",
    "    # PromptConfig(name=\"semi_lite\", template=prompts_data[\"human_prompt_context_semi_lite\"], requires_context=True),\n",
    "]\n",
    "\n",
    "# 2. Define qué modelos usar\n",
    "models_active = [\n",
    "    llm_gpt3,\n",
    "    llm_gpt4o,\n",
    "    llm_gpt5,\n",
    "    #llm_grok_adapter,\n",
    "    llm_deepseek_adapter,\n",
    "    llm_llama_adapter,\n",
    "] \n",
    "\n",
    "# --- B. FASE 1: Generación (LLM) ---\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "gen_file = f\"results/generations_{timestamp}.jsonl\"\n",
    "\n",
    "logging.info(\">>> INICIANDO FASE 1: GENERACIÓN LLM\")\n",
    "\n",
    "# AQUÍ defines el rango: (0, 10) procesa los primeros 10. (50, 100) procesa del 50 al 100.\n",
    "await run_generation_pipeline(\n",
    "    player_cards_list=player_cards_list,\n",
    "    models=models_active,\n",
    "    prompts_config=prompts_to_run,\n",
    "    user_range=(0, 2),   # <--- MODIFICAR RANGO AQUI\n",
    "    batch_size=10,       # Tamaño del lote asíncrono\n",
    "    output_filename=gen_file\n",
    ")\n",
    "\n",
    "# --- C. FASE 2: Evaluación (DeckShop) ---\n",
    "rating_file = f\"results/ratings_{timestamp}.jsonl\"\n",
    "\n",
    "logging.info(\">>> INICIANDO FASE 2: EVALUACIÓN DECKSHOP\")\n",
    "\n",
    "await run_rating_pipeline(\n",
    "    input_filename=gen_file,\n",
    "    output_filename=rating_file\n",
    ")\n",
    "\n",
    "logging.info(f\"Proceso completo. \\nGeneración: {gen_file} \\nRatings: {rating_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "96ed554b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = pd.read_json(\u001b[43moutput_filename\u001b[49m, orient=\u001b[33m\"\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m\"\u001b[39m, lines=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'output_filename' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(output_filename, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b95b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seleccion</th>\n",
       "      <th>user_id</th>\n",
       "      <th>deleted</th>\n",
       "      <th>original</th>\n",
       "      <th>llm</th>\n",
       "      <th>with_context</th>\n",
       "      <th>original_deck_rating</th>\n",
       "      <th>selected_deck_rating</th>\n",
       "      <th>total_original_deck_rating</th>\n",
       "      <th>total_selected_deck_rating</th>\n",
       "      <th>was_improved</th>\n",
       "      <th>correct_selection_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Goblin Barrel, The Log, Inferno Tower, Minions]</td>\n",
       "      <td>2G22QVP89</td>\n",
       "      <td>[Royal Recruits, Goblin Cage, Zappies, Arrows]</td>\n",
       "      <td>[Electro Spirit, Flying Machine, Golden Knight...</td>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Attack': 3, 'Defense': 5, 'Synergy': 1, 'Ver...</td>\n",
       "      <td>{'Attack': 4, 'Defense': 5, 'Synergy': 2, 'Ver...</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Tornado, Ice Golem, Skeletons, Cannon]</td>\n",
       "      <td>LU2QQJU0Y</td>\n",
       "      <td>[Musketeer, Minions, Mega Knight, Firecracker]</td>\n",
       "      <td>[Electro Wizard, Hog Rider, Executioner, The Log]</td>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Attack': 4, 'Defense': 5, 'Synergy': 4, 'Ver...</td>\n",
       "      <td>{'Attack': 2, 'Defense': 5, 'Synergy': 2, 'Ver...</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Poison, Skeletons, The Log, Goblin Gang]</td>\n",
       "      <td>999QV8RJU</td>\n",
       "      <td>[The Log, Goblin Gang, Zap, Mega Knight]</td>\n",
       "      <td>[Wall Breakers, Miner, Cannon, Bats]</td>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Attack': 4, 'Defense': 5, 'Synergy': 5, 'Ver...</td>\n",
       "      <td>{'Attack': 3, 'Defense': 5, 'Synergy': 4, 'Ver...</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Miner, Poison, Inferno Dragon, Goblin Gang]</td>\n",
       "      <td>VL0QCY9R8</td>\n",
       "      <td>[Zap, Mega Knight, Bandit, Spear Goblins]</td>\n",
       "      <td>[Firecracker, Bats, Skeletons, Skeleton Barrel]</td>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Attack': 3, 'Defense': 5, 'Synergy': 4, 'Ver...</td>\n",
       "      <td>{'Attack': 4, 'Defense': 5, 'Synergy': 5, 'Ver...</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Miner, Poison, Goblin Gang, Zap]</td>\n",
       "      <td>L0092LLGP</td>\n",
       "      <td>[Bandit, Skeleton Barrel, Zap, Firecracker]</td>\n",
       "      <td>[Mega Knight, Spear Goblins, Bats, Skeletons]</td>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Attack': 3, 'Defense': 5, 'Synergy': 4, 'Ver...</td>\n",
       "      <td>{'Attack': 3, 'Defense': 5, 'Synergy': 5, 'Ver...</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          seleccion    user_id  \\\n",
       "0  [Goblin Barrel, The Log, Inferno Tower, Minions]  2G22QVP89   \n",
       "1           [Tornado, Ice Golem, Skeletons, Cannon]  LU2QQJU0Y   \n",
       "2         [Poison, Skeletons, The Log, Goblin Gang]  999QV8RJU   \n",
       "3      [Miner, Poison, Inferno Dragon, Goblin Gang]  VL0QCY9R8   \n",
       "4                 [Miner, Poison, Goblin Gang, Zap]  L0092LLGP   \n",
       "\n",
       "                                          deleted  \\\n",
       "0  [Royal Recruits, Goblin Cage, Zappies, Arrows]   \n",
       "1  [Musketeer, Minions, Mega Knight, Firecracker]   \n",
       "2        [The Log, Goblin Gang, Zap, Mega Knight]   \n",
       "3       [Zap, Mega Knight, Bandit, Spear Goblins]   \n",
       "4     [Bandit, Skeleton Barrel, Zap, Firecracker]   \n",
       "\n",
       "                                            original            llm  \\\n",
       "0  [Electro Spirit, Flying Machine, Golden Knight...  DeepSeek-V3.1   \n",
       "1  [Electro Wizard, Hog Rider, Executioner, The Log]  DeepSeek-V3.1   \n",
       "2               [Wall Breakers, Miner, Cannon, Bats]  DeepSeek-V3.1   \n",
       "3    [Firecracker, Bats, Skeletons, Skeleton Barrel]  DeepSeek-V3.1   \n",
       "4      [Mega Knight, Spear Goblins, Bats, Skeletons]  DeepSeek-V3.1   \n",
       "\n",
       "   with_context                               original_deck_rating  \\\n",
       "0         False  {'Attack': 3, 'Defense': 5, 'Synergy': 1, 'Ver...   \n",
       "1         False  {'Attack': 4, 'Defense': 5, 'Synergy': 4, 'Ver...   \n",
       "2         False  {'Attack': 4, 'Defense': 5, 'Synergy': 5, 'Ver...   \n",
       "3          True  {'Attack': 3, 'Defense': 5, 'Synergy': 4, 'Ver...   \n",
       "4          True  {'Attack': 3, 'Defense': 5, 'Synergy': 4, 'Ver...   \n",
       "\n",
       "                                selected_deck_rating  \\\n",
       "0  {'Attack': 4, 'Defense': 5, 'Synergy': 2, 'Ver...   \n",
       "1  {'Attack': 2, 'Defense': 5, 'Synergy': 2, 'Ver...   \n",
       "2  {'Attack': 3, 'Defense': 5, 'Synergy': 4, 'Ver...   \n",
       "3  {'Attack': 4, 'Defense': 5, 'Synergy': 5, 'Ver...   \n",
       "4  {'Attack': 3, 'Defense': 5, 'Synergy': 5, 'Ver...   \n",
       "\n",
       "   total_original_deck_rating  total_selected_deck_rating  was_improved  \\\n",
       "0                          15                          18          True   \n",
       "1                          20                          14         False   \n",
       "2                          20                          17         False   \n",
       "3                          20                          22          True   \n",
       "4                          20                          19         False   \n",
       "\n",
       "   correct_selection_count  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        2  \n",
       "3                        0  \n",
       "4                        1  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['correct_selection_count'] = df.apply(\n",
    "    lambda row: sum(1 for item in row['deleted'] if item in row['seleccion']),\n",
    "    axis=1\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d396d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llm                     correct_selection_count\n",
       "DeepSeek-V3.1           0                          7\n",
       "                        1                          1\n",
       "                        2                          1\n",
       "                        3                          1\n",
       "Llama-3.3-70B-Instruct  0                          6\n",
       "                        1                          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['correct_selection_count'].groupby(df['llm']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcd967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      llm  total_mazos  mazos_mejorados  porcentaje_mejorados\n",
      "0           DeepSeek-V3.1           10                5             50.000000\n",
      "1  Llama-3.3-70B-Instruct            7                2             28.571429\n"
     ]
    }
   ],
   "source": [
    "res = (\n",
    "    df.assign(wi=lambda d: d[\"was_improved\"].astype(bool))\n",
    "      .groupby(\"llm\", as_index=False)\n",
    "      .agg(total_mazos=(\"llm\", \"size\"),\n",
    "           mazos_mejorados=(\"wi\", \"sum\"))\n",
    ")\n",
    "# Agregamos porcentaje de mazos mejorados\n",
    "res['porcentaje_mejorados'] = (res['mazos_mejorados'] / res['total_mazos']) * 100\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_json(raw_filename, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jv/rmw91d8n0g15ldpn_dkk73rh0000gn/T/ipykernel_96029/17924982.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_raw['is_valid'].groupby(df_raw['llm']).value_counts()\n",
      "/var/folders/jv/rmw91d8n0g15ldpn_dkk73rh0000gn/T/ipykernel_96029/17924982.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_raw.groupby('llm')['is_valid'].value_counts(normalize=True).mul(100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "llm                     is_valid\n",
       "DeepSeek-V3.1           False       50.0\n",
       "                        True        50.0\n",
       "Llama-3.3-70B-Instruct  False       65.0\n",
       "                        True        35.0\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contamos cuantos validos e invalidos hay en raw_data para cada llm\n",
    "df_raw['is_valid'] = df_raw['parsed_successfully'].astype(bool)\n",
    "df_raw['llm'] = df_raw['llm'].astype('category')\n",
    "df_raw['llm'].cat.categories\n",
    "df_raw['is_valid'].groupby(df_raw['llm']).value_counts()\n",
    "# Ponemos el porcentaje de validos por llm\n",
    "df_raw.groupby('llm')['is_valid'].value_counts(normalize=True).mul(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"seleccion\": [\n",
      "    \"Zap\",\n",
      "    \"Goblin Gang\",\n",
      "    \"Mega Knight\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "hola_serie = df_raw['raw_response'][(df_raw['user_id'] == '999QV8RJU') & (df_raw['llm'] == 'Llama-3.3-70B-Instruct')]\n",
    "\n",
    "if not hola_serie.empty:\n",
    "    texto_completo = hola_serie.iloc[0]\n",
    "    \n",
    "    print(texto_completo)\n",
    "else:\n",
    "    print(\"No se encontró esa fila.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
