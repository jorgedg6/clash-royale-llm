{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505b6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import re\n",
    "from random import shuffle\n",
    "import random\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d921ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from card_utils import read_all_game_data, Card, PlayerCards, PlayerStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f482727",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304bc2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deck_rate_fetcher import fetch_deck_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75129564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae66436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from openai import AsyncAzureOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1813383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_cards_list, player_cards_list, player_stats, player_tags_list = read_all_game_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e54480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_decks_per_card = pd.read_json(\"data/top_decks_per_card.json\").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d2a5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for card_name in top_decks_per_card:\n",
    "    for key in top_decks_per_card[card_name].keys():\n",
    "        deck_list = top_decks_per_card[card_name][key].split(',')\n",
    "        shuffle(deck_list)\n",
    "        top_decks_per_card[card_name][key] = deck_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e33d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LLMModel:\n",
    "    name: str\n",
    "    instance: AzureChatOpenAI | AsyncAzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54de7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_gpt3 = LLMModel(\n",
    "    name=\"gpt-35-turbo\",\n",
    "    instance=AzureChatOpenAI(\n",
    "        model_name=\"gpt-35-turbo\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "llm_gpt4o = LLMModel(\n",
    "    name=\"gpt-4o\",\n",
    "    instance=AzureChatOpenAI(\n",
    "        model_name=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "llm_gpt5 = LLMModel(\n",
    "    name=\"gpt-5-chat\",\n",
    "    instance=AzureChatOpenAI(\n",
    "        model_name=\"gpt-5-chat\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65334696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAIMessage:\n",
    "    \"\"\"Clase simple para empaquetar la respuesta con un atributo .content\"\"\"\n",
    "    def __init__(self, content, finish_reason=None):\n",
    "        self.content = content\n",
    "        self.finish_reason = finish_reason\n",
    "\n",
    "class OpenAIClientAdapter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: AsyncAzureOpenAI,\n",
    "        model_name: str,\n",
    "        temperature: float = 0,\n",
    "        max_tokens: int = 2048,\n",
    "        request_timeout: int = 300\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.request_timeout = request_timeout\n",
    "        print(f\"Adaptador creado para el modelo: {self.model_name}\")\n",
    "\n",
    "    def _convert_lc_messages_to_dict(self, messages):\n",
    "        output = []\n",
    "        for msg in messages:\n",
    "            role = msg.type\n",
    "            if role == \"human\":\n",
    "                role = \"user\"\n",
    "            elif role == \"ai\":\n",
    "                role = \"assistant\"\n",
    "            output.append({\"role\": role, \"content\": msg.content})\n",
    "        return output\n",
    "\n",
    "    async def ainvoke(self, messages):\n",
    "        try:\n",
    "            dict_messages = self._convert_lc_messages_to_dict(messages)\n",
    "            \n",
    "            resp = await self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=dict_messages,\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens,\n",
    "                extra_body={\"max_output_tokens\": self.max_tokens},\n",
    "                stop=None,\n",
    "                timeout=self.request_timeout,\n",
    "            )\n",
    "\n",
    "            choice = resp.choices[0]\n",
    "            content = choice.message.content\n",
    "            finish_reason = getattr(choice, \"finish_reason\", None)\n",
    "\n",
    "            if finish_reason and finish_reason != \"stop\":\n",
    "                print(f\"[{self.model_name}] finish_reason={finish_reason}\")\n",
    "\n",
    "            return SimpleAIMessage(content=content, finish_reason=finish_reason)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en el adaptador de OpenAI ({self.model_name}): {e}\")\n",
    "            return SimpleAIMessage(content=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95a8819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando clientes y adaptadores...\n",
      "Adaptador creado para el modelo: grok-4-fast-non-reasoning\n",
      "Adaptador creado para el modelo: DeepSeek-V3.1\n",
      "Adaptador creado para el modelo: Llama-3.3-70B-Instruct\n"
     ]
    }
   ],
   "source": [
    "print(\"Creando clientes y adaptadores...\")\n",
    "\n",
    "async_openai_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=dotenv.get_key(dotenv.find_dotenv(), \"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=dotenv.get_key(dotenv.find_dotenv(), \"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")\n",
    "\n",
    "llm_grok_adapter = LLMModel(\n",
    "    name=\"grok-4-fast-non-reasoning\",\n",
    "    instance=OpenAIClientAdapter(\n",
    "        client=async_openai_client,\n",
    "        model_name=\"grok-4-fast-non-reasoning\",\n",
    "        temperature=0\n",
    "    )\n",
    ")\n",
    "\n",
    "llm_deepseek_adapter = LLMModel(\n",
    "    name=\"DeepSeek-V3.1\",\n",
    "    instance=OpenAIClientAdapter(\n",
    "        client=async_openai_client,\n",
    "        model_name=\"DeepSeek-V3.1\",\n",
    "        temperature=0\n",
    "    )\n",
    ")\n",
    "\n",
    "llm_llama_adapter = LLMModel(\n",
    "    name=\"Llama-3.3-70B-Instruct\",\n",
    "    instance=OpenAIClientAdapter(\n",
    "        client=async_openai_client,\n",
    "        model_name=\"Llama-3.3-70B-Instruct\",\n",
    "        temperature=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b44cf5f",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b053b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-03 14:17:15,368 - root - INFO - Logger configurado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import sys\n",
    "\n",
    "LOG_LEVEL = logging.INFO \n",
    "\n",
    "logging.basicConfig(\n",
    "    level=LOG_LEVEL,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"logs/app_run{}.log\".format(time.strftime('%Y%m%d_%H%M%S'))), \n",
    "        logging.StreamHandler(sys.stdout) \n",
    "    ],\n",
    "    force=True \n",
    ")\n",
    "\n",
    "logging.info(\"Logger configurado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2c3d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompts/human_prompt_no_context.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    human_prompt_no_context = f.read().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    human_prompt_no_context = human_prompt_no_context.replace(\"\\n\", \" \").replace('\"', '\\\\\"')\n",
    "    human_prompt_no_context = human_prompt_no_context.replace(\"$\", \"{\").replace(\"%\", \"}\")\n",
    "\n",
    "with open(\"prompts/human_prompt_context.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    human_prompt_context = f.read().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    human_prompt_context = human_prompt_context.replace(\"\\n\", \" \").replace('\"', '\\\\\"')\n",
    "    human_prompt_context = human_prompt_context.replace(\"$\", \"{\").replace(\"%\", \"}\")\n",
    "\n",
    "with open(\"prompts/system_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    system_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a5d722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompts/human_prompt_no_context_lite.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    human_prompt_no_context_lite = f.read().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    human_prompt_no_context_lite = human_prompt_no_context_lite.replace(\"\\n\", \" \").replace('\"', '\\\\\"')\n",
    "    human_prompt_no_context_lite = human_prompt_no_context_lite.replace(\"$\", \"{\").replace(\"%\", \"}\")\n",
    "\n",
    "with open(\"prompts/human_prompt_context_lite.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    human_prompt_context_lite = f.read().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    human_prompt_context_lite = human_prompt_context_lite.replace(\"\\n\", \" \").replace('\"', '\\\\\"')\n",
    "    human_prompt_context_lite = human_prompt_context_lite.replace(\"$\", \"{\").replace(\"%\", \"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "337a7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompts/human_prompt_no_context_semi_lite.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    human_prompt_no_context_semi_lite = f.read().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    human_prompt_no_context_semi_lite = human_prompt_no_context_semi_lite.replace(\"\\n\", \" \").replace('\"', '\\\\\"')\n",
    "    human_prompt_no_context_semi_lite = human_prompt_no_context_semi_lite.replace(\"$\", \"{\").replace(\"%\", \"}\")\n",
    "\n",
    "with open(\"prompts/human_prompt_context_semi_lite.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    human_prompt_context_semi_lite = f.read().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    human_prompt_context_semi_lite = human_prompt_context_semi_lite.replace(\"\\n\", \" \").replace('\"', '\\\\\"')\n",
    "    human_prompt_context_semi_lite = human_prompt_context_semi_lite.replace(\"$\", \"{\").replace(\"%\", \"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72b6d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mapping = {\n",
    "    \"RIP\": 0,\n",
    "    \"Bad\": 1,\n",
    "    \"Mediocre\": 2,\n",
    "    \"Good\": 3,\n",
    "    \"Great!\": 4,\n",
    "    \"Godly!\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07a38d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_deck_rating(rating: str) -> dict:\n",
    "    rating = rating.strip()\n",
    "    rating = rating.split(\" \")\n",
    "    ratings = {\n",
    "        \"Attack\": score_mapping.get(rating[1], -1),\n",
    "        \"Defense\": score_mapping.get(rating[3], -1),\n",
    "        \"Synergy\": score_mapping.get(rating[5], -1),\n",
    "        \"Versatility\": score_mapping.get(rating[7], -1),\n",
    "        \"F2P score\": score_mapping.get(rating[10], -1),\n",
    "    }\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a152709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def parse_json_safe(player_cards, llm_response, llm_name, with_context: bool):\n",
    "    player_tag = player_cards.tag[1:]\n",
    "    deck_cards = player_cards.deck_cards\n",
    "    deleted_cards = player_cards.deleted_cards or []\n",
    "\n",
    "    try:\n",
    "        s = (llm_response or \"\").strip()\n",
    "\n",
    "        m = re.search(r\"```(?:json)?\\\\s*([\\\\s\\\\S]*?)\\\\s*```\", s, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            candidate = m.group(1).strip()\n",
    "        else:\n",
    "            candidate = s\n",
    "\n",
    "        first_brace = candidate.find(\"{\")\n",
    "        first_bracket = candidate.find(\"[\")\n",
    "        starts = [i for i in (first_brace, first_bracket) if i != -1]\n",
    "        \n",
    "        if not starts:\n",
    "            error_msg = f\"! Error de parseo (Usuario: {player_tag}, LLM: {llm_name}, Ctx: {with_context}): No se encontró JSON.\"\n",
    "            logging.warning(error_msg)\n",
    "            return None, error_msg\n",
    "\n",
    "        start = min(starts)\n",
    "        end = max(candidate.rfind(\"}\"), candidate.rfind(\"]\"))\n",
    "        \n",
    "        if end == -1 or end < start:\n",
    "            error_msg = f\"! Error de parseo (Usuario: {player_tag}, LLM: {llm_name}, Ctx: {with_context}): JSON mal formado.\"\n",
    "            logging.warning(error_msg)\n",
    "            return None, error_msg\n",
    "            \n",
    "        candidate = candidate[start:end+1]\n",
    "        parsed = json.loads(candidate)\n",
    "\n",
    "        if isinstance(parsed, dict):\n",
    "            parsed[\"user_id\"] = player_tag\n",
    "            parsed[\"deleted\"] = list(map(lambda c: c.name, deleted_cards))\n",
    "            parsed[\"original\"] = list(map(lambda c: c.name, deck_cards))\n",
    "            parsed[\"llm\"] = llm_name\n",
    "            parsed[\"with_context\"] = with_context\n",
    "        else:\n",
    "            parsed = {\n",
    "                \"user_id\": player_tag,\n",
    "                \"data\": parsed,\n",
    "                \"deleted\": list(map(lambda c: c.name, deleted_cards)),\n",
    "                \"llm\": llm_name,\n",
    "                \"with_context\": with_context\n",
    "            }\n",
    "\n",
    "        original_deck_rating_string = fetch_deck_rating(parsed[\"original\"] + parsed[\"deleted\"])\n",
    "        selected_deck_rating_string = fetch_deck_rating(parsed[\"original\"] + parsed[\"seleccion\"])\n",
    "\n",
    "        original_deck_ratings = process_deck_rating(original_deck_rating_string)\n",
    "        selected_deck_ratings = process_deck_rating(selected_deck_rating_string)\n",
    "\n",
    "        parsed[\"original_deck_rating\"] = original_deck_ratings\n",
    "        parsed[\"selected_deck_rating\"] = selected_deck_ratings\n",
    "\n",
    "        total_original_deck_rating = sum(original_deck_ratings.values())\n",
    "        total_selected_deck_rating = sum(selected_deck_ratings.values())\n",
    "\n",
    "        parsed[\"total_original_deck_rating\"] = total_original_deck_rating\n",
    "        parsed[\"total_selected_deck_rating\"] = total_selected_deck_rating\n",
    "\n",
    "        parsed[\"was_improved\"] = total_selected_deck_rating >= total_original_deck_rating\n",
    "\n",
    "        return parsed, None\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        error_msg = f\"X Error JSONDecode (Usuario: {player_tag}, LLM: {llm_name}, Ctx: {with_context}): {e}\"\n",
    "        logging.error(error_msg)\n",
    "        return None, error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"! Error inesperado (Usuario: {player_tag}, LLM: {llm_name}, Ctx: {with_context}): {e}\"\n",
    "        logging.error(error_msg)\n",
    "        return None, error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdd813e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_prompt_file(string, user_id):\n",
    "    with open(f\"final_prompts_lite/user_{user_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc08d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_user_recommendation(\n",
    "    llm, \n",
    "    player_cards, \n",
    "    system_prompt_str, \n",
    "    human_prompt_context_str,\n",
    "    human_prompt_no_context_str,\n",
    "    with_context: bool,\n",
    "    valid_f,\n",
    "    raw_f,\n",
    "    file_lock\n",
    "):\n",
    "    player_tag = player_cards.tag[1:]\n",
    "    available_cards = player_cards.available_cards\n",
    "    deck_cards = player_cards.deck_cards\n",
    "\n",
    "    available_cards_json = json.dumps(list(map(lambda c: c.name, available_cards)))\n",
    "    deck_cards_json = json.dumps(list(map(lambda c: c.name, deck_cards)))\n",
    "\n",
    "    try:\n",
    "        if with_context:\n",
    "            string_top_decks = \"<Top 5 Decks para usar con cada una de las CARTAS_DISPONIBLES>\\\\n\"\n",
    "            \n",
    "            for card in deck_cards:\n",
    "                card_name = card.name\n",
    "                string_top_decks += f'  <CardGroup name=\"{card_name}\">\\\\n'\n",
    "                decks_for_card = top_decks_per_card.get(card_name, {}) \n",
    "                for deck_key in decks_for_card.keys():\n",
    "                    deck_list = decks_for_card[deck_key]\n",
    "                    string_top_decks += \"    <Deck>\\\\n\"\n",
    "                    for individual_card_name in deck_list:\n",
    "                        string_top_decks += f'      <Card>{individual_card_name.strip()}</Card>\\\\n'\n",
    "                    string_top_decks += \"    </Deck>\\\\n\"\n",
    "                string_top_decks += f'  </CardGroup>\\\\n'\n",
    "            string_top_decks += \"</TopDecks>\"\n",
    "\n",
    "            rendered_human_prompt = human_prompt_context_str.format(\n",
    "                TOP_DECKS=string_top_decks,\n",
    "                CARTAS_DISPONIBLES=available_cards_json,\n",
    "                CARTAS_SELECCIONADAS=deck_cards_json\n",
    "            )\n",
    "        else:\n",
    "            rendered_human_prompt = human_prompt_no_context_str.format(\n",
    "                CARTAS_DISPONIBLES=available_cards_json,\n",
    "                CARTAS_SELECCIONADAS=deck_cards_json\n",
    "            )\n",
    "        \n",
    "        prompt_filename_tag = f\"{player_tag}{'_ctx' if with_context else '_noctx'}\"\n",
    "        write_prompt_file(rendered_human_prompt, prompt_filename_tag)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"! Error al renderizar prompt para usuario {player_tag} (Ctx: {with_context}): {e}\")\n",
    "        return False\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt_str),\n",
    "        HumanMessage(content=rendered_human_prompt),\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        ai_msg = await llm.instance.ainvoke(messages)\n",
    "        response_content = ai_msg.content\n",
    "\n",
    "        parsed_result, parse_error = parse_json_safe(player_cards, response_content, llm.name, with_context)\n",
    "        \n",
    "        raw_log_data = {\n",
    "            \"user_id\": player_tag,\n",
    "            \"llm\": llm.name,\n",
    "            \"with_context\": with_context,\n",
    "            \"raw_response\": response_content,\n",
    "            \"parsed_successfully\": (parsed_result is not None),\n",
    "            \"parse_error\": parse_error\n",
    "        }\n",
    "\n",
    "        async with file_lock:\n",
    "            raw_f.write(json.dumps(raw_log_data, ensure_ascii=False) + '\\n')\n",
    "            raw_f.flush()\n",
    "            \n",
    "            if parsed_result:\n",
    "                valid_f.write(json.dumps(parsed_result, ensure_ascii=False) + '\\n')\n",
    "                valid_f.flush()\n",
    "\n",
    "        if parsed_result:\n",
    "            logging.info(f\"Éxito: Usuario {player_tag} (LLM: {llm.name}, Ctx: {with_context})\")\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"! Error API (Usuario: {player_tag}, LLM: {llm.name}, Ctx: {with_context}): {e}\")\n",
    "        raw_log_data = {\n",
    "            \"user_id\": player_tag,\n",
    "            \"llm\": llm.name,\n",
    "            \"with_context\": with_context,\n",
    "            \"raw_response\": None,\n",
    "            \"parsed_successfully\": False,\n",
    "            \"parse_error\": f\"API Error: {e}\"\n",
    "        }\n",
    "        async with file_lock:\n",
    "            raw_f.write(json.dumps(raw_log_data, ensure_ascii=False) + '\\n')\n",
    "            raw_f.flush()\n",
    "            \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8956854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_human_prompt(human_prompt_template, top_decks, available_cards, selected_cards):\n",
    "    rendered_human_prompt = human_prompt_template.format(\n",
    "        TOP_DECKS=top_decks,\n",
    "        CARTAS_DISPONIBLES=available_cards,\n",
    "        CARTAS_SELECCIONADAS=selected_cards\n",
    "    )\n",
    "    return rendered_human_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c5ee3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_remove_cards(player_cards_list, num_to_remove=4):\n",
    "    updated_player_cards_list = []\n",
    "    for player_cards in player_cards_list:\n",
    "        \n",
    "        available_copy = list(player_cards.available_cards)\n",
    "        selected_copy = list(player_cards.deck_cards)\n",
    "\n",
    "        shuffle(available_copy)\n",
    "        shuffle(selected_copy)\n",
    "\n",
    "        to_delete = selected_copy[-num_to_remove:]\n",
    "        selected_for_prompt = selected_copy[:-num_to_remove]\n",
    "\n",
    "        updated_player_cards = PlayerCards(\n",
    "            tag=player_cards.tag,\n",
    "            available_cards=available_copy,\n",
    "            deck_cards=selected_for_prompt,\n",
    "            deleted_cards=to_delete\n",
    "        )\n",
    "        updated_player_cards_list.append(updated_player_cards)\n",
    "    \n",
    "    return updated_player_cards_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd1649cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "async def run_all_models_async(\n",
    "    player_cards_list, \n",
    "    models_list,\n",
    "    system_prompt_str, \n",
    "    human_prompt_context_str,\n",
    "    human_prompt_no_context_str,\n",
    "    batch_size=50, \n",
    "    num_batches=None,\n",
    "    starting_card_number=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Orquesta la ejecución asíncrona para todos los LLMs y usuarios,\n",
    "    controlando el tamaño de los lotes y guardando los resultados en JSONL.\n",
    "    \"\"\"\n",
    "    selected_player_cards_list = shuffle_and_remove_cards(\n",
    "        player_cards_list, \n",
    "        num_to_remove= 8 - starting_card_number\n",
    "    )\n",
    "\n",
    "    if num_batches is not None:\n",
    "        total_to_process = batch_size * num_batches\n",
    "        players_to_process = selected_player_cards_list[:total_to_process]\n",
    "    else:\n",
    "        players_to_process = selected_player_cards_list\n",
    "\n",
    "    logging.info(f\"Iniciando procesamiento para {len(players_to_process)} jugadores a través de {len(models_list)} LLMs (x2 con/sin contexto).\")\n",
    "\n",
    "    output_timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "    valid_results_filename = f\"results/valid_results_{output_timestamp}.jsonl\"\n",
    "    raw_results_filename = f\"results/raw_results_{output_timestamp}.jsonl\"\n",
    "    \n",
    "    file_lock = asyncio.Lock()\n",
    "    \n",
    "    with open(valid_results_filename, 'a', encoding='utf-8') as valid_f, \\\n",
    "         open(raw_results_filename, 'a', encoding='utf-8') as raw_f:\n",
    "\n",
    "        all_tasks = []\n",
    "        for llm in models_list:\n",
    "            logging.info(f\"Creando tareas para: {llm.name}...\")\n",
    "            for player_cards in players_to_process:\n",
    "                for with_context_flag in [True, False]:\n",
    "                    task = process_user_recommendation(\n",
    "                        llm,\n",
    "                        player_cards, \n",
    "                        system_prompt_str, \n",
    "                        human_prompt_context_str,\n",
    "                        human_prompt_no_context_str,\n",
    "                        with_context_flag,\n",
    "                        valid_f,\n",
    "                        raw_f,\n",
    "                        file_lock\n",
    "                    )\n",
    "                    all_tasks.append(task)\n",
    "                \n",
    "        logging.info(f\"Total de tareas creadas: {len(all_tasks)}\")\n",
    "        \n",
    "        all_results = []\n",
    "        total_batches = (len(all_tasks) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for i in range(0, len(all_tasks), batch_size):\n",
    "            current_batch_num = (i // batch_size) + 1\n",
    "            batch_tasks = all_tasks[i:i + batch_size]\n",
    "            logging.info(f\"\\n--- Ejecutando Lote {current_batch_num} / {total_batches} (Tamaño: {len(batch_tasks)}) ---\")\n",
    "            \n",
    "            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)\n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            for idx, result in enumerate(batch_results):\n",
    "                if isinstance(result, Exception):\n",
    "                    logging.error(f\"! Excepción en tarea del lote {current_batch_num}: {result}\")\n",
    "            \n",
    "            await asyncio.sleep(1) \n",
    "\n",
    "    logging.info(\"\\n--- Procesamiento de lotes completado. ---\")\n",
    "    \n",
    "    valid_count = sum(1 for r in all_results if r is True)\n",
    "    logging.info(f\"Resultados válidos (parseados y guardados): {valid_count} de {len(all_tasks)}\")\n",
    "\n",
    "    return valid_results_filename, raw_results_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a95912e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-03 14:17:15,417 - root - INFO - Iniciando la ejecución asíncrona...\n",
      "2025-11-03 14:17:16,423 - root - INFO - Iniciando procesamiento para 10 jugadores a través de 2 LLMs (x2 con/sin contexto).\n",
      "2025-11-03 14:17:16,423 - root - INFO - Creando tareas para: gpt-35-turbo...\n",
      "2025-11-03 14:17:16,424 - root - INFO - Creando tareas para: Llama-3.3-70B-Instruct...\n",
      "2025-11-03 14:17:16,424 - root - INFO - Total de tareas creadas: 40\n",
      "2025-11-03 14:17:16,424 - root - INFO - \n",
      "--- Ejecutando Lote 1 / 4 (Tamaño: 10) ---\n",
      "2025-11-03 14:17:22,370 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-03 14:17:22,524 - root - ERROR - ! Error inesperado (Usuario: VL0QCY9R8, LLM: gpt-35-turbo, Ctx: True): 403 Client Error: Forbidden for url: https://www.deckshop.pro/check/?deck=Firecracker-Bats-Skeletons-SkeletonBarrel-Zap-MegaKnight-Bandit-SpearGoblins\n",
      "2025-11-03 14:17:23,298 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-03 14:17:23,398 - root - ERROR - ! Error inesperado (Usuario: L0092LLGP, LLM: gpt-35-turbo, Ctx: False): 403 Client Error: Forbidden for url: https://www.deckshop.pro/check/?deck=MegaKnight-SpearGoblins-Bats-Skeletons-Bandit-SkeletonBarrel-Zap-Firecracker\n",
      "2025-11-03 14:17:23,401 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-03 14:17:23,500 - root - ERROR - ! Error inesperado (Usuario: 2G22QVP89, LLM: gpt-35-turbo, Ctx: True): 403 Client Error: Forbidden for url: https://www.deckshop.pro/check/?deck=ElectroSpirit-FlyingMachine-GoldenKnight-SkeletonBarrel-RoyalRecruits-GoblinCage-Zappies-Arrows\n",
      "2025-11-03 14:17:23,559 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-03 14:17:23,643 - root - ERROR - ! Error inesperado (Usuario: VL0QCY9R8, LLM: gpt-35-turbo, Ctx: False): 403 Client Error: Forbidden for url: https://www.deckshop.pro/check/?deck=Firecracker-Bats-Skeletons-SkeletonBarrel-Zap-MegaKnight-Bandit-SpearGoblins\n",
      "2025-11-03 14:17:23,646 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-03 14:17:23,647 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-03 14:17:23,751 - root - ERROR - ! Error inesperado (Usuario: L0092LLGP, LLM: gpt-35-turbo, Ctx: True): 403 Client Error: Forbidden for url: https://www.deckshop.pro/check/?deck=MegaKnight-SpearGoblins-Bats-Skeletons-Bandit-SkeletonBarrel-Zap-Firecracker\n",
      "2025-11-03 14:17:23,840 - root - ERROR - ! Error inesperado (Usuario: LU2QQJU0Y, LLM: gpt-35-turbo, Ctx: True): 403 Client Error: Forbidden for url: https://www.deckshop.pro/check/?deck=ElectroWizard-HogRider-Executioner-TheLog-Musketeer-Minions-MegaKnight-Firecracker\n",
      "2025-11-03 14:17:23,843 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-03 14:17:23,845 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-03 14:17:23,942 - root - ERROR - ! Error inesperado (Usuario: 2G22QVP89, LLM: gpt-35-turbo, Ctx: False): 403 Client Error: Forbidden for url: https://www.deckshop.pro/check/?deck=ElectroSpirit-FlyingMachine-GoldenKnight-SkeletonBarrel-RoyalRecruits-GoblinCage-Zappies-Arrows\n",
      "2025-11-03 14:17:24,025 - root - ERROR - ! Error inesperado (Usuario: LU2QQJU0Y, LLM: gpt-35-turbo, Ctx: False): 403 Client Error: Forbidden for url: https://www.deckshop.pro/check/?deck=ElectroWizard-HogRider-Executioner-TheLog-Musketeer-Minions-MegaKnight-Firecracker\n",
      "2025-11-03 14:17:24,028 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-03 14:17:24,109 - root - ERROR - ! Error inesperado (Usuario: 999QV8RJU, LLM: gpt-35-turbo, Ctx: False): 403 Client Error: Forbidden for url: https://www.deckshop.pro/check/?deck=WallBreakers-Miner-Cannon-Bats-TheLog-GoblinGang-Zap-MegaKnight\n",
      "2025-11-03 14:17:26,945 - httpx - INFO - HTTP Request: POST https://victo-mhcmsfx4-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-11-03 14:17:27,035 - root - ERROR - ! Error inesperado (Usuario: 999QV8RJU, LLM: gpt-35-turbo, Ctx: True): 403 Client Error: Forbidden for url: https://www.deckshop.pro/check/?deck=WallBreakers-Miner-Cannon-Bats-TheLog-GoblinGang-Zap-MegaKnight\n",
      "2025-11-03 14:17:28,038 - root - INFO - \n",
      "--- Ejecutando Lote 2 / 4 (Tamaño: 10) ---\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m NUM_BATCHES = \u001b[32m1\u001b[39m\n\u001b[32m     15\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mIniciando la ejecución asíncrona...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m valid_filename, raw_filename = \u001b[38;5;28;01mawait\u001b[39;00m run_all_models_async(\n\u001b[32m     18\u001b[39m     player_cards_list=player_cards_list,\n\u001b[32m     19\u001b[39m     models_list=models_to_run,\n\u001b[32m     20\u001b[39m     system_prompt_str=system_prompt,\n\u001b[32m     21\u001b[39m     human_prompt_context_str=human_prompt_context,\n\u001b[32m     22\u001b[39m     human_prompt_no_context_str=human_prompt_no_context,\n\u001b[32m     23\u001b[39m     batch_size=BATCH_SIZE,\n\u001b[32m     24\u001b[39m     num_batches=NUM_BATCHES\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEjecución finalizada.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResultados válidos guardados en: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mrun_all_models_async\u001b[39m\u001b[34m(player_cards_list, models_list, system_prompt_str, human_prompt_context_str, human_prompt_no_context_str, batch_size, num_batches, starting_card_number)\u001b[39m\n\u001b[32m     64\u001b[39m batch_tasks = all_tasks[i:i + batch_size]\n\u001b[32m     65\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Ejecutando Lote \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_batch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (Tamaño: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch_tasks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m batch_results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*batch_tasks, return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     68\u001b[39m all_results.extend(batch_results)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/asyncio/tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/asyncio/tasks.py:306\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    304\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    309\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mprocess_user_recommendation\u001b[39m\u001b[34m(llm, player_cards, system_prompt_str, human_prompt_context_str, human_prompt_no_context_str, with_context, valid_f, raw_f, file_lock)\u001b[39m\n\u001b[32m     54\u001b[39m messages = [\n\u001b[32m     55\u001b[39m     SystemMessage(content=system_prompt_str),\n\u001b[32m     56\u001b[39m     HumanMessage(content=rendered_human_prompt),\n\u001b[32m     57\u001b[39m ]\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     ai_msg = \u001b[38;5;28;01mawait\u001b[39;00m llm.instance.ainvoke(messages)\n\u001b[32m     61\u001b[39m     response_content = ai_msg.content\n\u001b[32m     63\u001b[39m     parsed_result, parse_error = parse_json_safe(player_cards, response_content, llm.name, with_context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    394\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m     **kwargs: Any,\n\u001b[32m    400\u001b[39m ) -> AIMessage:\n\u001b[32m    401\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    403\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    404\u001b[39m         stop=stop,\n\u001b[32m    405\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    406\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    407\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    408\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    409\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    410\u001b[39m         **kwargs,\n\u001b[32m    411\u001b[39m     )\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    413\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m, cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n\u001b[32m    414\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1099\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1092\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1096\u001b[39m     **kwargs: Any,\n\u001b[32m   1097\u001b[39m ) -> LLMResult:\n\u001b[32m   1098\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1099\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1100\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1101\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1006\u001b[39m run_managers = \u001b[38;5;28;01mawait\u001b[39;00m callback_manager.on_chat_model_start(\n\u001b[32m   1007\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   1008\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1013\u001b[39m     run_id=run_id,\n\u001b[32m   1014\u001b[39m )\n\u001b[32m   1016\u001b[39m input_messages = [\n\u001b[32m   1017\u001b[39m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m   1018\u001b[39m ]\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1020\u001b[39m     *[\n\u001b[32m   1021\u001b[39m         \u001b[38;5;28mself\u001b[39m._agenerate_with_cache(\n\u001b[32m   1022\u001b[39m             m,\n\u001b[32m   1023\u001b[39m             stop=stop,\n\u001b[32m   1024\u001b[39m             run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1025\u001b[39m             **kwargs,\n\u001b[32m   1026\u001b[39m         )\n\u001b[32m   1027\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages)\n\u001b[32m   1028\u001b[39m     ],\n\u001b[32m   1029\u001b[39m     return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1030\u001b[39m )\n\u001b[32m   1031\u001b[39m exceptions = []\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/asyncio/tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/asyncio/tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1310\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1308\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1311\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1312\u001b[39m     )\n\u001b[32m   1313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1314\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1539\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1532\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1533\u001b[39m             response,\n\u001b[32m   1534\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1535\u001b[39m             metadata=generation_info,\n\u001b[32m   1536\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1537\u001b[39m         )\n\u001b[32m   1538\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1539\u001b[39m         raw_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.with_raw_response.create(\n\u001b[32m   1540\u001b[39m             **payload\n\u001b[32m   1541\u001b[39m         )\n\u001b[32m   1542\u001b[39m         response = raw_response.parse()\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/openai/_legacy_response.py:381\u001b[39m, in \u001b[36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    377\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:2603\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2557\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2558\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2559\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2600\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2601\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2602\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2604\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2605\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2606\u001b[39m             {\n\u001b[32m   2607\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2608\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2609\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2610\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2611\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2612\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2613\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2614\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2615\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2616\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2617\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2618\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2620\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2621\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2622\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2623\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2625\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2626\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2627\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2628\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2629\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2630\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2631\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2632\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2633\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2634\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2635\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2636\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2637\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2638\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2639\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2640\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2641\u001b[39m             },\n\u001b[32m   2642\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2643\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2644\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2645\u001b[39m         ),\n\u001b[32m   2646\u001b[39m         options=make_request_options(\n\u001b[32m   2647\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2648\u001b[39m         ),\n\u001b[32m   2649\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2650\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2651\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2652\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/openai/_base_client.py:1529\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1527\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1528\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1530\u001b[39m         request,\n\u001b[32m   1531\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1532\u001b[39m         **kwargs,\n\u001b[32m   1533\u001b[39m     )\n\u001b[32m   1534\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1535\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    399\u001b[39m     status_code=resp.status,\n\u001b[32m    400\u001b[39m     headers=resp.headers,\n\u001b[32m    401\u001b[39m     stream=AsyncResponseStream(resp.stream),\n\u001b[32m    402\u001b[39m     extensions=resp.extensions,\n\u001b[32m    403\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = \u001b[38;5;28;01mawait\u001b[39;00m pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpcore/_async/connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpcore/_async/http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpcore/_async/http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpcore/_async/http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/anyio/streams/tls.py:237\u001b[39m, in \u001b[36mTLSStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m = \u001b[32m65536\u001b[39m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_sslobject_method(\u001b[38;5;28mself\u001b[39m._ssl_object.read, max_bytes)\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/anyio/streams/tls.py:180\u001b[39m, in \u001b[36mTLSStream._call_sslobject_method\u001b[39m\u001b[34m(self, func, *args)\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._write_bio.pending:\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.send(\u001b[38;5;28mself\u001b[39m._write_bio.read())\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.receive()\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[32m    182\u001b[39m     \u001b[38;5;28mself\u001b[39m._read_bio.write_eof()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/site-packages/anyio/_backends/_asyncio.py:1263\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1258\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1261\u001b[39m ):\n\u001b[32m   1262\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/asyncio/locks.py:213\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/asyncio/futures.py:286\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    285\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/asyncio/tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/recsys-llm/lib/python3.13/asyncio/futures.py:194\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the result this future represents.\u001b[39;00m\n\u001b[32m    188\u001b[39m \n\u001b[32m    189\u001b[39m \u001b[33;03mIf the future has been cancelled, raises CancelledError.  If the\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mfuture's result isn't yet available, raises InvalidStateError.  If\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mthe future is done and has an exception set, this exception is raised.\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == _CANCELLED:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_cancelled_error()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state != _FINISHED:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.InvalidStateError(\u001b[33m'\u001b[39m\u001b[33mResult is not ready.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "models_to_run = [\n",
    "    llm_gpt3,\n",
    "    #llm_gpt4o,\n",
    "    #llm_gpt5,\n",
    "    #llm_grok_adapter,\n",
    "    #llm_deepseek_adapter,\n",
    "    llm_llama_adapter,\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "NUM_BATCHES = 1\n",
    "\n",
    "logging.info(\"Iniciando la ejecución asíncrona...\")\n",
    "\n",
    "valid_filename, raw_filename = await run_all_models_async(\n",
    "    player_cards_list=player_cards_list,\n",
    "    models_list=models_to_run,\n",
    "    system_prompt_str=system_prompt,\n",
    "    human_prompt_context_str=human_prompt_context,\n",
    "    human_prompt_no_context_str=human_prompt_no_context,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches=NUM_BATCHES\n",
    ")\n",
    "\n",
    "logging.info(f\"Ejecución finalizada.\")\n",
    "logging.info(f\"Resultados válidos guardados en: '{valid_filename}'\")\n",
    "logging.info(f\"Resultados crudos guardados en: '{raw_filename}'\")\n",
    "\n",
    "output_filename = valid_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(output_filename, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b95b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seleccion</th>\n",
       "      <th>user_id</th>\n",
       "      <th>deleted</th>\n",
       "      <th>original</th>\n",
       "      <th>llm</th>\n",
       "      <th>with_context</th>\n",
       "      <th>original_deck_rating</th>\n",
       "      <th>selected_deck_rating</th>\n",
       "      <th>total_original_deck_rating</th>\n",
       "      <th>total_selected_deck_rating</th>\n",
       "      <th>was_improved</th>\n",
       "      <th>correct_selection_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Goblin Barrel, The Log, Inferno Tower, Minions]</td>\n",
       "      <td>2G22QVP89</td>\n",
       "      <td>[Royal Recruits, Goblin Cage, Zappies, Arrows]</td>\n",
       "      <td>[Electro Spirit, Flying Machine, Golden Knight...</td>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Attack': 3, 'Defense': 5, 'Synergy': 1, 'Ver...</td>\n",
       "      <td>{'Attack': 4, 'Defense': 5, 'Synergy': 2, 'Ver...</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Tornado, Ice Golem, Skeletons, Cannon]</td>\n",
       "      <td>LU2QQJU0Y</td>\n",
       "      <td>[Musketeer, Minions, Mega Knight, Firecracker]</td>\n",
       "      <td>[Electro Wizard, Hog Rider, Executioner, The Log]</td>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Attack': 4, 'Defense': 5, 'Synergy': 4, 'Ver...</td>\n",
       "      <td>{'Attack': 2, 'Defense': 5, 'Synergy': 2, 'Ver...</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Poison, Skeletons, The Log, Goblin Gang]</td>\n",
       "      <td>999QV8RJU</td>\n",
       "      <td>[The Log, Goblin Gang, Zap, Mega Knight]</td>\n",
       "      <td>[Wall Breakers, Miner, Cannon, Bats]</td>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Attack': 4, 'Defense': 5, 'Synergy': 5, 'Ver...</td>\n",
       "      <td>{'Attack': 3, 'Defense': 5, 'Synergy': 4, 'Ver...</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Miner, Poison, Inferno Dragon, Goblin Gang]</td>\n",
       "      <td>VL0QCY9R8</td>\n",
       "      <td>[Zap, Mega Knight, Bandit, Spear Goblins]</td>\n",
       "      <td>[Firecracker, Bats, Skeletons, Skeleton Barrel]</td>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Attack': 3, 'Defense': 5, 'Synergy': 4, 'Ver...</td>\n",
       "      <td>{'Attack': 4, 'Defense': 5, 'Synergy': 5, 'Ver...</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Miner, Poison, Goblin Gang, Zap]</td>\n",
       "      <td>L0092LLGP</td>\n",
       "      <td>[Bandit, Skeleton Barrel, Zap, Firecracker]</td>\n",
       "      <td>[Mega Knight, Spear Goblins, Bats, Skeletons]</td>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Attack': 3, 'Defense': 5, 'Synergy': 4, 'Ver...</td>\n",
       "      <td>{'Attack': 3, 'Defense': 5, 'Synergy': 5, 'Ver...</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          seleccion    user_id  \\\n",
       "0  [Goblin Barrel, The Log, Inferno Tower, Minions]  2G22QVP89   \n",
       "1           [Tornado, Ice Golem, Skeletons, Cannon]  LU2QQJU0Y   \n",
       "2         [Poison, Skeletons, The Log, Goblin Gang]  999QV8RJU   \n",
       "3      [Miner, Poison, Inferno Dragon, Goblin Gang]  VL0QCY9R8   \n",
       "4                 [Miner, Poison, Goblin Gang, Zap]  L0092LLGP   \n",
       "\n",
       "                                          deleted  \\\n",
       "0  [Royal Recruits, Goblin Cage, Zappies, Arrows]   \n",
       "1  [Musketeer, Minions, Mega Knight, Firecracker]   \n",
       "2        [The Log, Goblin Gang, Zap, Mega Knight]   \n",
       "3       [Zap, Mega Knight, Bandit, Spear Goblins]   \n",
       "4     [Bandit, Skeleton Barrel, Zap, Firecracker]   \n",
       "\n",
       "                                            original            llm  \\\n",
       "0  [Electro Spirit, Flying Machine, Golden Knight...  DeepSeek-V3.1   \n",
       "1  [Electro Wizard, Hog Rider, Executioner, The Log]  DeepSeek-V3.1   \n",
       "2               [Wall Breakers, Miner, Cannon, Bats]  DeepSeek-V3.1   \n",
       "3    [Firecracker, Bats, Skeletons, Skeleton Barrel]  DeepSeek-V3.1   \n",
       "4      [Mega Knight, Spear Goblins, Bats, Skeletons]  DeepSeek-V3.1   \n",
       "\n",
       "   with_context                               original_deck_rating  \\\n",
       "0         False  {'Attack': 3, 'Defense': 5, 'Synergy': 1, 'Ver...   \n",
       "1         False  {'Attack': 4, 'Defense': 5, 'Synergy': 4, 'Ver...   \n",
       "2         False  {'Attack': 4, 'Defense': 5, 'Synergy': 5, 'Ver...   \n",
       "3          True  {'Attack': 3, 'Defense': 5, 'Synergy': 4, 'Ver...   \n",
       "4          True  {'Attack': 3, 'Defense': 5, 'Synergy': 4, 'Ver...   \n",
       "\n",
       "                                selected_deck_rating  \\\n",
       "0  {'Attack': 4, 'Defense': 5, 'Synergy': 2, 'Ver...   \n",
       "1  {'Attack': 2, 'Defense': 5, 'Synergy': 2, 'Ver...   \n",
       "2  {'Attack': 3, 'Defense': 5, 'Synergy': 4, 'Ver...   \n",
       "3  {'Attack': 4, 'Defense': 5, 'Synergy': 5, 'Ver...   \n",
       "4  {'Attack': 3, 'Defense': 5, 'Synergy': 5, 'Ver...   \n",
       "\n",
       "   total_original_deck_rating  total_selected_deck_rating  was_improved  \\\n",
       "0                          15                          18          True   \n",
       "1                          20                          14         False   \n",
       "2                          20                          17         False   \n",
       "3                          20                          22          True   \n",
       "4                          20                          19         False   \n",
       "\n",
       "   correct_selection_count  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        2  \n",
       "3                        0  \n",
       "4                        1  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['correct_selection_count'] = df.apply(\n",
    "    lambda row: sum(1 for item in row['deleted'] if item in row['seleccion']),\n",
    "    axis=1\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d396d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llm                     correct_selection_count\n",
       "DeepSeek-V3.1           0                          7\n",
       "                        1                          1\n",
       "                        2                          1\n",
       "                        3                          1\n",
       "Llama-3.3-70B-Instruct  0                          6\n",
       "                        1                          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['correct_selection_count'].groupby(df['llm']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcd967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      llm  total_mazos  mazos_mejorados  porcentaje_mejorados\n",
      "0           DeepSeek-V3.1           10                5             50.000000\n",
      "1  Llama-3.3-70B-Instruct            7                2             28.571429\n"
     ]
    }
   ],
   "source": [
    "res = (\n",
    "    df.assign(wi=lambda d: d[\"was_improved\"].astype(bool))\n",
    "      .groupby(\"llm\", as_index=False)\n",
    "      .agg(total_mazos=(\"llm\", \"size\"),\n",
    "           mazos_mejorados=(\"wi\", \"sum\"))\n",
    ")\n",
    "# Agregamos porcentaje de mazos mejorados\n",
    "res['porcentaje_mejorados'] = (res['mazos_mejorados'] / res['total_mazos']) * 100\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_json(raw_filename, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jv/rmw91d8n0g15ldpn_dkk73rh0000gn/T/ipykernel_96029/17924982.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_raw['is_valid'].groupby(df_raw['llm']).value_counts()\n",
      "/var/folders/jv/rmw91d8n0g15ldpn_dkk73rh0000gn/T/ipykernel_96029/17924982.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_raw.groupby('llm')['is_valid'].value_counts(normalize=True).mul(100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "llm                     is_valid\n",
       "DeepSeek-V3.1           False       50.0\n",
       "                        True        50.0\n",
       "Llama-3.3-70B-Instruct  False       65.0\n",
       "                        True        35.0\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contamos cuantos validos e invalidos hay en raw_data para cada llm\n",
    "df_raw['is_valid'] = df_raw['parsed_successfully'].astype(bool)\n",
    "df_raw['llm'] = df_raw['llm'].astype('category')\n",
    "df_raw['llm'].cat.categories\n",
    "df_raw['is_valid'].groupby(df_raw['llm']).value_counts()\n",
    "# Ponemos el porcentaje de validos por llm\n",
    "df_raw.groupby('llm')['is_valid'].value_counts(normalize=True).mul(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"seleccion\": [\n",
      "    \"Zap\",\n",
      "    \"Goblin Gang\",\n",
      "    \"Mega Knight\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "hola_serie = df_raw['raw_response'][(df_raw['user_id'] == '999QV8RJU') & (df_raw['llm'] == 'Llama-3.3-70B-Instruct')]\n",
    "\n",
    "if not hola_serie.empty:\n",
    "    texto_completo = hola_serie.iloc[0]\n",
    "    \n",
    "    print(texto_completo)\n",
    "else:\n",
    "    print(\"No se encontró esa fila.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
